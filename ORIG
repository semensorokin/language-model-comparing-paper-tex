[PREDICT] Atricle Outline

Predictability Project - Article Structure

Abstract[a]

Problem Statement

Indication of Methodology

Main Findings

Principal Conclusion

Introduction

Literature overview[b]

Previous research in the field [Laurinavichyute, A.K. et al., 2018] Anastasia

What hasn’t been researched/solved in previous research → gaps:

There are similar studies for other languages (German etc.), but none for Russian. Daria

The task of replacing the cloze task with a neural network (i.e. task of creating a cloze task model) is new. Daria

How the present study will cover the gap Anastasia

Methodology

3.1. Description of Cloze task dataset Anastasia

3.2. Corpus-based language models and their architecture Sergey

3.3. Corpora Sergey

3.4. Overview of used metrics. Semen

        Mean accuracy

Absolute number of correct word predictions

        Context consistency

        Kolmagorov-Smirnov test

        Kullback–Leibler divergence Sergey

3.5. PoS tagging. Daria

3.6. Object - Modifier -  Verb - Functional tagging. Darya

3.7. Semantic comparison Darya

Results

4.1. Quantitative and qualitative analysis of cloze task language model Semen

4.2. Model comparison on the Lexical Level Semen, Sergey, Daria

4.3. Model comparison on the PoS level Semen, Anastasia

4.4. Model comparison on the OMVF level Semen,  Daria

Discussion[c] Daria & Anastasia

Acknowledgements

References

Bibliography

Introduction Daria & Anastasia

Nowadays it is an adopted practice to measure predictability of words by carrying out elaborate cloze test experiments. It is an exercise[d] or assessment consisting of a portion of language with certain items, words, or signs removed. The participant is asked to replace the missing language item. Based on cloze test results, it is possible to calculate the probability score of predicting the next word in relation to the left context.[e] As cloze tasks require a major time commitment and is financially expensive, the goal of this study is to find out whether the actual human respondents can be replaced by an artificial language model trained on a big corpus. Language models originally were created for solving similar tasks as parts of complex linguistic systems, such as machine translation engines, text generators, summarization systems, etc.

Literature Overview Daria & Anastasia

As Kuperberg and Jaeger claim in their “What do we mean by prediction in language comprehension?” [2016], the reaction time is in direct ratio with the predictability of the word: the more predictable the word is the faster is the reaction time. Moreover, predictability of a word or a context[f]  defines[g] fixation time in eye-movement studies as a result of language comprehension process. This implies that language comprehension must[h] be predictive. The authors also state that as the previous context expands, the predictability of the next word increases leading to - in cloze tests - higher accuracy of predicting the next work, and - in eye-movement experiments - to shorter fixation duration[i]. 

3. Methodology

3.1 . Cloze Probabilities Anastasia

We used the dataset with cloze task answers from the Laurinavichyute et al. [2018]  study. This dataset is based on 144 sentences randomly selected from the Russian National Corpus (RNC,  ruscorpora.ru) - an online corpus of Russian texts with extensive search options. These sentences were slightly edited: the authors replaced rare infrequent words with more frequent ones and shortened the sentences when they exceeded the preset maximum length of 13 words. The stimuli sentences were subjected to the cloze task experiment. Cloze task [Taylor, 1953] is a test, commonly carried out for assessing native speakers of a language, which is aimed to understand respondents’ comprehension of a language and their ability to predict missing portions of written texts [Laurinavichyute et al. 2018]. This experiment presumes that native speakers are able to understand context and vocabulary in order to identify the correct semantic field or part of speech of a missing word.[j] In the Laurinavichyute et al. study, the respondents were asked to successively predict next words for each context. An example of stimuli that were shown to the participants is presented in the table below with corresponding correct next words and calculated predictability scores.

#

Stimuli

Correct word

Predictability

1

А

промывать

1,99E-07

2

А промывать

манную

9,95E-06

3

А промывать манную

крупу

0,091529563

4

А промывать манную крупу

перед

0,000779675

5

А промывать манную крупу перед

тем

0,015035226

6

А промывать манную крупу перед тем‚

как

0,966154218

7

А промывать манную крупу перед тем‚ как

варить

0,011867962

8

А промывать манную крупу перед тем‚ как варить

ее

0,04090891

9

А промывать манную крупу перед тем‚ как варить ее‚

не

0,146951959

10

А промывать манную крупу перед тем‚ как варить ее‚ не

пробовали

0,000829122

Each context received from [~10] to [~100] responses, not all of which matched the correct word. The predictability of each next word was computed as the number of correctly predicted words divided by the total number of predicted words. The Laurinavichyute et al. article and the full list of sentences used in the study can be found. 

3.2. Corpus-based probabilities

For computing сorpus-based probabilities, different model types and training corpora were selected. The goal of these combinations is to represent some dependencies (if they exist) between model architecture,  vocabulary and to compare results.

Models[k]

In this research we were solving the task of language modeling - task of predicting the next word given the corpus. There are several models that perform well of this task type, including HMM, LSTM and BERT. We used pretrained models on our data to predict next word for each context. These models were trained on different corpora to see how corpora influence model performance.

Hidden Markov Models

Markov chain theory is increasingly used in real-world computing applications as it provides a convenient way to capture pattern dependencies in pattern recognition systems. For this reason, Markov chain theory is suitable for natural language processing (NLP), where data consists of repeating sequences of symbols or words. 

In this case, we are using bi- and tri-grams HMM not for PoS-tagging but for prediction of the next word. To eliminate out-of-vocabulary errors in our HMM models, we will use Good-Turing smoothing.

LSTM

A one-layer long short-term memory (LSTM) recurrent neural network model was used [Jozefowicz, Vinyals, Schuster, Shazeer, Wu, 2016] to create a list of predictions for each word in the same 144 stimuli sentences. The dimensions of the model are 2048 for hidden layer and 512 for the input and output layers. 

BERT

Bidirectional Encoder Representations from Transformers (BERT) is trained on a masked language modeling objective. Unlike a traditional language modeling objective of predicting the next word in a sequence given the history, masked language modeling predicts a word given its left and right context. Because the model expects context from both directions, it is not obvious how BERT can be used as a traditional language model (i.e. to evaluate the probability of a text sequence) or how to sample from it. We test several ideas: give the model all of content, except masked word, and using the technique[] to rework BERT as classical language model.[l]

For experiments we will using BERT trained on Russian Wiki corpus []. To show differences  between models, we will fine-tune BERT with our corpuses.[m][n]

3. 3. Corpora[o] Sergey, Semen

Russian news corpus includes newspaper articles published in the 2000s: newspaper different types of articles.[p] 

National corpus of Russian language include written texts from the middle of the 18th to the middle of the 20th centuries[q].

Some corpus characteristics presented below:

Name        

Texts        

Size in GB        

Mean of text length, words        

Russian news corpus (RNC)

470304                

2,928        

176                

National corpus of Russian language        (NCRL)

110708

3,210

2341

The sum of the two corpuses above

581012

6138

1258

All of our models (except LSTM) was trained separately on 2 corpora and  their combination. Thus we examine these models:

Bigram HMM on RNC

Bigram HMM on NCRL

Trigram HMM on RNC

Trigram HMM on NCRL

BERT[r]

BERT fine-tuned on RNC

BERT fine-tuned on NCRL

BERT fine-tuned on RNC and NCRL

LSTM[s]

Bigram HMM on NCRL + BERT fine-tuned on NCRL

LSTM was trained on the part of RNC[t], excluding the sentences chosen for stimuli, namely on modern written texts from the 1950s to the present day, real-life Russian oral speech recordings from the same period. In addition[u], it was trained on RNC and NCRL. Overall, the training corpus consisted of 577 million tokens. The model was tested on 1000 sentences from the OpenCorpora [http://opencorpora.org/; Bocharov[v] et al. 2011] with overall 1,9 million of tokens from newspaper articles, Russian Wikipedia, texts from blogs, fiction, non-fiction, and legal documents. 

Among all, there is a Bigram HMM on NCRL + BERT fine-tuned on NCRL model, which is by structure a combination of a bigram HMM and a BERT model. These models were joined based on the best performance of both models: probability distributions of HMM are used for contexts with length less than 6 tokens, and BERT is used for longer contexts. 

Renormalization of probabilities

To evaluate each model’s predictions, we took the first 30 most probable words. Each probability was renormalized by dividing originally computed word-wise probability by the sum of probabilities of the first 30 words. This way the sum of probabilities the selected words would equal 1.

3.4. Overview of used metrics

        

Mean accuracy Semen

 

The metric is used to compute the mean of correct word prediction across all contexts.  Range of values from 0 to 1. It was computed as a mean value of the array of accuracies.

[w]

Absolute number of correct word predictions Semen

The metric represents the amount of contexts for each prediction of the correct word is non-zero.    [x]

Context consistency Semen

The metric represents the proportion of “context consistency”. It can be interpreted as the answer to the next question: “How many contexts coincide assuming that prediction of the correct word (for each of them) is not equal to zero for a certain model pair?

[y]

Kolmogorov-Smirnov test Semen

In our study, we used two-sample Kolmogorov-Smirnov test to find out whether two underlying one-dimensional probability distributions of model predictions differ. The null hypothesis of Kolmogorov-Smirnov test is ￼: both samples of predicted words come from a population with the same probability distribution. 

The Kolmogorov–Smirnov statistic is 

￼

{D_{n,m}=\sup _{x}|F_{1,n}(x)-F_{2,m}(x)|,}[z]

where {F_{1,n}} and  {F_{2,m}} are the empirical distribution functions of the first and the second sample respectively, and {\sup} is the supremum function. The null hypothesis is rejected at level {\alpha } if 

￼

{D_{n,m}>c(\alpha ){\sqrt {\frac {n+m}{nm}}},}

where {n} and {m} are the sizes of first and second sample respectively, and where c(α) is the inverse of the Kolmorogov distribution at α, which can be calculated as 

￼

{ c\left(\alpha \right)={\sqrt {-{\frac {1}{2}}\ln \alpha }}.}

The advantage of Kolmogorov-Smirnov test is that, unlike t-test, it can catch the difference between Gaussian distributions with similar means but different variances.

This metric was used to compare both lexical and word class[aa] probability distributions of cloze task, LSTM, HMM models pairwise. The results of the metric are listed in the Results section of this article.

Kullback–Leibler divergence Sergey

Cross-entropy loss, or log loss, measures the performance of a classification[ab] model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. The cross-entropy shows the difference between probability distributions p and q. Kullback and Leibler defined a similar measure now known as KL divergence. This measure quantifies how similar a probability distribution p is to a candidate distribution q. We used KL-divergence to compare several[ac] language model.

3.6. PoS[ad][ae]  probabilities Daria

Word[af] class probabilities were computed as follows: each word in the model’s[ag] vocabulary (N = 500000 most frequent words in the training corpus) and each word in the stimuli sentences was tagged for word class and morphological features using PyMorphy2 analyser [Korobov[ah], 2015] and the predictions of the model were compared to the annotation of the target words. A word class match was coded if the predicted and target word belonged to the same word class. The probability of a word class was computed by summing probabilities of all words in the model’s vocabulary which had the morphosyntactic feature in question. For example, to estimate the word class probability in a sentence “A mobile ______”, where "phone" is the target word, we would sum up probabilities of all nouns in the model’s vocabulary. For morphologically ambiguous words (e.g., рот ‘mouth’ in nominative or accusative singular), all possible variants were considered in the probability estimation.

3.7. Probabilities for OBJECT-VERB-FUNCTIONAL-MODIFIER Daria

We have also tried to use different tags for our part-of-speech tagging. Instead of using all of the tags, we thought we could use a more generalized set of object, verb, modifier and functional word, because when a person mentally chooses the next word, they might not think in terms of the usual parts of speech, but choose generally an object, or a description of an object, or a verb, or just some functional word.

Firstly, we converted all of the modified Pymorphy tags into 4 general sets: 'ADJ', 'ADVB', 'NUMR' were generalized to 'MOD'; 'INFN' and 'PRED' - to 'VERB'; 'NPRO' to 'NOUN'; 'PREP', 'PRCL', 'CONJ' and 'INTJ' to 'FUNC' for each context. Then, we have counted probabilities of these tags in the same manner as the usual parts of speech[ai]. 

We noticed that the generalized probabilities were overall higher than with modified Pymorphy tags.

3.8.  Semantic comparison Daria

After lots of trial and error[aj], it was decided to use semantic vectors for the comparison of cloze task with other models in the semantic aspect[ak]. All words were mapped into a vector space of the model pre-trained on Wikipedia texts[al] [Arefyev et al., 2015]. The comparison itself needed to be dynamic, because for each context a different amount of words should have been chosen.

To compare semantic vectors for each context, firstly, we cleared all of the words so there would be no digits, meaningless letters and punctuation. Then, we have built a function[am], which:

1)        Extracts first 10 words (we have decided that that is the maximum amount of words for each context that would have meaningful probabilities as all of the words after the first ten for each model have their probabilities tends to zero) for each of 1219 contexts[an];

2)        Counts the quotient of the sum of words’ probabilities and each amount of words (for example, if two words were to have probabilities 0.5 and 0.1, we would count (0.5+0.1)/2), which we call mean probabilities);

3)        Counts the difference between the probabilities of the first and last word[ao] and then difference between mean probabilities of the previous word and the next one[ap];

4)        Decides what amount of words vectors to use for each context based on how different the first-last word difference and the mean probability difference is – if the latter is lesser than the former, the function checks the next difference, if not – the previous amount of words would be used for semantic comparison.

After the decision on the amount of words was made, with the help of gensim in Python a vector for each word was extracted.

Then, MiniBatchKmeans (a modified version of the K-means algorithm, that uses mini-batches to reduce computation time, while at the same time trying to optimize the same goal function [Béjar, 2010]) algorithm in sklearn was used to find “cluster centers”, or mean semantic vector for one model for each context[aq]. And at last, we found cosine similarity (also with sklearn library) for each pair of semantic vectors for each pair of models.

Cosine similarity  Anastasia

Cosine similarity was used to measure closeness of semantic vectors of predicted words between different models.

Cosine similarity is a measure of similarity between two non-zero vectors that measures the cosine of the angle between them. Given two vectors of attributes, A and B, the cosine similarity, cos(θ), is calculated as

￼

{{\text{similarity}}=\cos(\theta )={{A} \cdot {B} \over \|{A} \|\|{B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},} 

where {A_{i}} and {B_{i}} are components of vector {A} and {B} respectively.

The cosine similarity is widely used for calculating distance between two words. In our study, cosine similarity was used to find semantic probability of predicting the word which is semantically close to the target word. 

Results

4.1. Quantitative and qualitative analysis of cloze task language model Semen

First of all, it is necessary to establish how many different answers on average are available for each context in a language model based on a cloze task, since it is necessary to determine how many of the most likely words will be explored in artificial language models and test the hypothesis about the dependence of predictability on the length of the context.

Mean quantity of predictions for the language model built on cloze-task results is 17 words.

￼[ar]

Minimum variance in context filling by one lexical unit is observed in the following contexts:

Какие главные лекарства должны входить в

В современном обществе семья и школа оказывают большое влияние

Зачем ему звонить если откликается спокойный женский голос

Ирине досталась отдельная комната в двухкомнатной квартире

Они не ели целый день

Во избежание ожогов надо нанести на лицо небольшое количество

Во избежание ожогов надо нанести на лицо небольшое количество защитного крема

Дрозды и скворцы начали вить семейные

Дрозды и скворцы начали вить семейные гнезда неподалеку друг

Дрозды и скворцы начали вить семейные гнезда неподалеку друг от друга

Собаку виновницу случившегося приказали сечь хотя в чем была ее вина

С нескрываемой едкой иронией отзываются они друг о

С нескрываемой едкой иронией отзываются они друг о друге

Олень бродил среди берез жевал талый снег и

It is worth emphasizing that in all the above cases there was no variance in respondents’ answers, i.e. all respondents gave the same one answer for these contexts. What is more, this predicted word was the original word from the corpus.

We classified these contexts based on their constraining ability[as] and found out the following groups of com:

Semantically constraining contexts [contexts #3, 4, 6, 7, 8, 9]

Syntactically constraining contexts [contexts #1, 14]

Idiomatically constraining contexts [contexts #2, 5, 10, 11, 12, 13]

The maximum number of different answers were received for the “на болотах” (“On the Swamps”) context - 87 words, which is explained by the absence of any limiting semantic properties of the context.

In this regard, the study of the artificial language model distribution is meaningless, as it will always be uniform, so as to say, the indicator for each context in similar histograms will be equal to the size of the vocabulary.￼

Another important aspect in the study of the linguistic model of the cloze task is the relationship between the length of the context and the probability of predicting [=predictability of] the correct word.

This graph reflects a high value of predictability for contexts of length both less and more than five lexical units. However, the lack of correlation is worth emphasizing. We received Pearson correlation coefficient score of 0.323 and Spearman correlation coefficient of 0.363.￼

According to the results of our experiment, the closest probability distribution of the correct word of all the models was achieved with BERT trained on the literary[at] corpus.

In this case[au], there is practically no correlation[av]: Pearson correlation score is 0.093521, Spearman correlation - 0.079094.

￼￼

[aw]

Each model was evaluated by two measures: mean accuracy of model predictions and absolute number of correct word predictions. For computing mean accuracy, the mean of correct answer probabilities was taken. In case of absolute number of correct word predictions, a model achieved +1 score if there was at least one correct answer among all predictions.[ax]

4.2. Model comparison on the Lexical Level Semen

The[ay][az] bar chart below shows the mean accuracy scores of each model on the lexical level. As the goal of our study was to build an algorithm, which would be the closest approximation of the cloze task results (18% accuracy), we can see that BERT {not a language model one} model scored better than the others. Interestingly, all HMM model architectures showed low results on the lexical level.￼

It is noticeable that BERT mean accuracy results are higher than close task score. This can be explained by the fact that the model was trained on a large amount of written texts and thus had a higher chance to guess the correct word. Following this assumption, an assumption can be made that respondents’ active vocabulary size is lower than model’s vocabulary. Also, we can make a hypothesis that the process of word retrieval by humans and by the model is performed differently, as respondents do not always respond with the most probable answer[ba].￼

2. In terms of absolute number of predicted words, in the cloze task around 625 contexts were given at least one correct prediction. The closest to that are the results of the bigram HMM model combined with BERT (with around 545 contexts with at least one correct prediction) and raw BERT (with about 725 contexts with at least one correct prediction).￼

3. Next, we compared models’ performances using an inclusion-exclusion principle to find a percentage of overlapping answers between different models. The result of this comparison is shown on the heat map below.

The heat map reflects information on pairwise model comparison, however we are mainly interested in how close the models are to the cloze task model. The comparison showed that the models with the largest overlap with the cloze task are birgram HMM model combined with BERT (58% of overlap) and BERT trained on RNC.

4. At the next step, we performed Kolmogorov-Smirnov testing to find the similarity in the probability distributions of the model predictions. This table also reflects the sum value of pairwise comparison of the contexts using Kolmogorov-Smirnov testing. For convenience all the values were normalized. Observations show that the closest probability distribution is seen in LSTM model. However, as we see, there is a variation in metric values from 0.9 to 6.2 for different models. Thus, we can assert that the difference between all models and a cloze task is rather large and in some way unacceptable.[bb]

￼

5.To measure model predictions on semantic level, the cosine similarity between each context’s predicted words centroid vector was found. The number of words was selected dynamically for each context by maximizing vector significance with the minimum words.

[bc]

￼

6. KL distance Sergey￼

Due to the fact that all of our models are word-level, and in order to lower the casing variability we've combined all our vocabularies into one[bd]. For this compound vocabulary we calculated the KL distance of our models.

The table below shows the scores for three different models with bigram HMM trained on NCRL showing the best results. Unfortunately, this heat map does not show us changes in LM {language model?} distances context-lengthwise. Table of top-3 of lengthwise LM distances.

Context length        

Bigram HMM (NCRL)

BERT        (NCRL)        

LSTM

1

1.338877707133317

2.169391719510728

2.010368022161614

2        

1.572140987811664

2.0953176467055203

1.773435391050547

3

1.8354864696851096

2.0747792268074523

1.8353423966902578

4

1.789022273199865

1.934784246744554

1.809010678254156

5

1.9096451822913887

2.016769763173693

1.9157414802302832

6

1.8558933217941342

1.881661221559258

1.87256448537585

7

1.9872357018327926

1.7714025771610524

1.8866070874247325

8        

1.7284021098042532

1.6888599895961085

1.7905836065437595

9        

1.9704235071818408

1.5535781276967953

1.6301257491708843

10        

2.461873281116166

1.7143595169629797

1.8025697746290807

11

2.7906425492820075

2.6810193094792467

2.1743528115039794

As we see, the bigram models starts to higher the distance from 1 to 6 context length, but BERT on the contrary start to lower the distance up to 6 context length. For this case we merged bigram model and BERT at border of length 6.

KL(Cloze || Cloze) = 0.0

KL(Cloze || HMM) = 1.7915

KL(Cloze || BERT) = 1.9320

KL(Cloze || LSTM) = 1.8427

KL(Cloze || HMM + BERT) = 1.7110

4.3. Model comparison on the PoS level Semen, Anastasia

1. Mean Accuracy of Models on the Part-of-Speech Level

The performance of all models, except for LSTM[be][bf][bg], at the parts of speech level has significantly and proportionally increased. This is due to a decrease in the set of classes for which classification occurs. At this stage, the first 30 words of each model were tagged for parts of speech. Overall, there were 16 word classes. It is notable that BERT linguistic models have the highest scores.

￼

2. In absolute values, there is also a tendency in higher accuracy of BERT models, which can be interpreted as follows. BERT as a language model (lm) correctly predicts a part of the next word, but the words themselves, rather close to the context, have a low probability. Moreover, in many cases, they have almost a uniform distribution equal to 0.033.

￼

3. The consistency of a part of speech prediction differs significantly from the lexical level. However, when compared with the cloze task model, we do not see a strong resemblance with one of the artificial models. That is, there are contexts for which a person is able to predict the part of speech of the next word correctly, while the models are not able to do the same.

￼

4.4. Model comparison on the OMVF level Semen, Daria

Reducing the number of classes by 4 times does not lead to an improvement in the average accuracy[bh]. Although we can see that the models perform similar as on the original parts of speech model.

￼

2. Absolute values at OMVF level of generalization cease to reflect any properties of the models. This is due to the metric calculation algorithm. The model's indicator increases by one each time when there is a correct answer and its probability is not equal to 0. Accordingly, the graph reflects that in more than 95% of cases the correct tag is present. It can be noted that for a model with a random tag generator, this threshold would be 25%. Such differences in magnitudes suggest productivity at the OMVF level. 

￼

￼

KS test (based on absolute value statistics)

￼

The last two tests reflect a contradictory trend: LSTM shows a greater resemblance to the cloze task. Here it is necessary to make a comment about probabilistic distribution. Calculation of statistical tests based on probability distributions of the models makes the metrics far from objective, since the variation of the number of lexical units to consider significantly impacts the final output. Thus the results may highly differ for 25 and 30 lexical units. Moreover, artificial models allow us to reflect distributions for the large vocabulary, whereas the vocabulary of the cloze task is very limited and has many random outliers when the context is not constraining the next word on any level. It is disputable whether such cases should be eliminated from the vocabulary during research or not.                          

Bibliography[bi] 

Kuperberg, G.R., Jaeger, T.F. (2016) What do we mean by prediction in language comprehension? Language, Cognition and Neuroscience, 31, 32-59.

Laurinavichyute, A.K., Sekerina, I.A., Alexeeva, S. et al. (2018) Russian Sentence Corpus: Benchmark measures of eye movements in reading in Russian. Behavior Research Methods,  https://doi.org/10.3758/s13428-018-1051-6.

Staub, A., Grant, M., Astheimer, L., Cohen A. (2015) The influence of cloze probability and item constraint on cloze task response time, Journal of Memory and Language, 82, 1-17.

Hofmann, M.J., Biemann, C., Remus, S. (2017) Benchmarking n-grams, Topic Models and Recurrent Neural Networks by Cloze Completions, EEGs and Eye Movements. ResearchGate, 1-17.

Smith, N.J., Levy, R. (2011) Cloze but no cigar: The complex relationship between cloze, corpus, and subjective probabilities in language processing. Cognitive Science Society, 33, 1637-1642.

Frisson, S., Harvey, D.R., Staub, A. (2017) No prediction error cost in reading: Evidence from eye movements. Journal of Memory and Language, 95, 200-214.

Luke, S.G., Christianson, K. (2016) Limits on lexical prediction during reading. Cognitive Psychology, 88, 22-60.

Murgue, T., Higuera, C. (2004) Distances Between Distributions: Comparing Language Models. Structural, Syntactic, and Statistical Pattern Recognition, 2004, 269-277.

Halser, E., Stahlberg, V., Tomalin, M., et al. (2017) A Comparison of Neural Models for Word Ordering. ACL Anthology, 2017, 208-212.

Chen, S., Beeferman, D., Rosenfeld, R. (1998) Evaluation Metrics for Language Models. Carnegie Mellon University, 1998, 1-6.

Al-Anzi F.S., Abu Zeina, D. (2017) Statistical Markovian Data Modeling for Natural Language Processing. International Journal of Data Mining & Knowledge Management Process, Vol. 7, No. 1, 25-35.

Arefyev N.V., Panchenko A.I., Lukanin A.V., Lesota O.O., Romanov P.V. Evaluating Three Corpus based Semantic Similarity Systems for Russian // Компьютерная лингвистика и интеллектуальные технологии: По материалам ежегодной Международной конференции Диалог (Москва, 27 — 30 мая 2015 г.). — Вып. 14 (21). — Т. 2. — Изд-во РГГУ Москва, 2015. — С. 116–128.

Béjar, J. (2010) K-means vs Mini Batch K-means: A comparison. Universitat Politècnica de Catalunya, https://upcommons.upc.edu/bitstream/handle/2117/ 23414/R13-8.pdf

[a]no abstract

[b]it is very limited in the text

[c]it is not present in the text...

[d]task?

[e]not clear, how probability score is calculated

[f]??? can we measure predictability of a context?

[g]too strong

explains variance

[h]other papers show that it is not an oblagation

[i]incorrect implication

[j]does it?

[k]what is the rationale behind all these models?

Why did you choose them?

[l]not clear: how did you plan to retransform BERT?

[m]how this sentence is related to the previous one?

[n]Непонятно: в итоге BERT у вас видит контекст справа или нет?

[o]why did you select these particular corpora?

[p]link

[q]and what about later texts?

[r]what corpus it was trained on?

[s]trained on what?

[t]did you train your own lstm?

[u]why?

[v]add to the references

[w]how was it computed

[x]how was it computed

[y]how was it computed

[z]latex format for the final article

[aa]what is it? it was not introduced above

[ab]which of the models was a classification model? it was not explained above

[ac]unclear

[ad]what it the rationale to compute Pos probabilities?

[ae]PoS or word class? choose one nomination

[af]it is my text

[ag]did you do this for the cloze task responses?

[ah]references

[ai]did you do this for models' predictions or for cloze responses as well?

[aj]what did you try? can you list it?

[ak]what does it mean? what was the goal?

[al]why did you choose this model???

[am]какова идея за этой функцией? вы можете сначала объяснить, что вы в принципе хотели сделать?

[an]how did you get this number?

[ao]where? in the context?

[ap]in context?

[aq]for each context or for each word in context?

why did you measure semantic similarity of contexts?

[ar]number of guesses for each target word?

[as]More info on constraining contexts with reference to the original article

[at]which one?

[au]not clear

[av]between what and what?

[aw]do this figures illustrate BERT??

[ax]not clear

[ay]I am surprised with the lstm results

In our study we got 19% accuracy for lstm trained on the national corpus of Russian

[az]https://colab.research.google.com/drive/1vZ4a2T-cCpE76VGvw4E-IyODy7XapWkc Добрый день, здесь выложил часть кода, отвечающую за расчет метрики для ЛСТМ,  не могли бы сверить со своими результатами?

[ba]это довольно странное заключение: что мы считаем наиболее вероятным ответом? Что у нас является золотым стандартом вероятности ответа? Почему ответ BERT более вероятный, чем ответ людей?

[bb]why? what are the acceptability criteria?

[bc]what does the heatmap reflect?

[bd]why did you decide to do this?

[be]this is a really strange result

Can you double-check it?

[bf]and it contradicts your calculations below (3)

[bg]Непонятно, как PoS accuracy вообще может быть ниже word accuracy: если модель правильно предсказала слово, то она и его часть речи правильно предсказала.

[bh]this is also very strange

Are you absolutely sure about these results? How can you explain them?

[bi]should be aligned with the text

